{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d5909b-96b2-455d-b0d5-e1aa24cee733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32520a6-eddb-4a0f-9436-1bd549d09365",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example of Significance Power Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7513f36f-a204-4e8c-82fe-4c7029f55c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_sample_size(metric, mde, alpha, beta): # mde = minimum detectable effect, metric for us is CTR so 0.33\n",
    "    # standard normal distribution to determine z-values\n",
    "    snd = stats.norm(0, 1)\n",
    "\n",
    "    Z_beta = snd.ppf(1-beta)\n",
    "    print(Z_beta)\n",
    "\n",
    "    Z_alpha = snd.ppf(1-alpha/2)\n",
    "    print(Z_alpha)\n",
    "\n",
    "    # average of probabilities from both groups\n",
    "    p = (metric + metric+mde) / 2\n",
    "    print(p)\n",
    "\n",
    "    N = (2 * p * \n",
    "             (1 - p) * \n",
    "             (0.84 + 1.96)**2\n",
    "             / mde**2)\n",
    "\n",
    "    return N # difference between amemnding third line to variable or not is we get precise value if put specific values in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52395a5-b95f-4e79-a741-69ca49006e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8416212335729143\n",
      "1.959963984540054\n",
      "0.34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8796.479999999998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binomial_sample_size(metric=0.33, mde=0.02, alpha=0.05, beta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2f6358-268d-435a-821d-ea33a4a4fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuos_sample_size(metric, mde, sd, alpha, beta):\n",
    "    # standard normal distribution to determine z-values\n",
    "    snd = stats.norm(0, 1)\n",
    "\n",
    "    Z_beta = snd.ppf(1-beta)\n",
    "    print(Z_beta)\n",
    "\n",
    "    Z_alpha = snd.ppf(1-alpha/2)\n",
    "    print(Z_alpha)\n",
    "\n",
    "    N = (2 * sd**2 * \n",
    "             (Z_beta + Z_alpha)**2\n",
    "             / mde**2)\n",
    "\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e609d98-accd-4451-ad8c-df479249f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8416212335729143\n",
      "1.959963984540054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4443682906698845"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuos_sample_size(metric=30673, mde=300, sd=91, alpha=0.05, beta=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389e86a",
   "metadata": {},
   "source": [
    "## Test prep- Finalising the decision on how to run the A/B Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e662d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Hypothesis is defined\n",
    "# 2. success and non-inferiority metrics are set and we can track them\n",
    "# 3. We agreed a desired significance and power levels for the test\n",
    "# 4. We can reach the signficance and power in a reasonable timeframe (2 days based on no of users)\n",
    "# 5. We've agreed on a halt criteria for the test during the monitoring stage (if no of crashes is too high, test will stop)\n",
    "# 6. Duration of the test also includes the ability to measure long term metrics if needed (retention, weekly active users etc)\n",
    "## 7. We've ensured that there are no other tests/releases that could interfere with the test or the other way around \n",
    "## which test could interfere with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a47e92",
   "metadata": {},
   "source": [
    "## Assignment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20afb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually a randomised test assignment is done based on user_id\n",
    "# There needs to be no pre-test bias between the groups \n",
    "## Pre-test bias means that users are not randomly shuffled between the groups \n",
    "## and one of the groups end up with a significantly different mean to any tracked metric \n",
    "## to avoid pre-test bias companies run pre-assignment analysis, \n",
    "## where they 'assign' (but not really) users randomly based on  multiple different seeds \n",
    "## and calculate difference in the metrics  \n",
    "# then company select the seed with no potential bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43699c85",
   "metadata": {},
   "source": [
    "## Performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "## monitor the test performance on critical metrics while it is running \n",
    "## to be able to prevent a potential negative impact on the users and the business \n",
    "# we don't yet determine if test is successful or not\n",
    "## we monitor alarming trends, like increase in crashes, drop in engagement, \n",
    "## and may want to pause the test and investigate issues\n",
    "## There could also be a potential negative reaction from users, \n",
    "## so it is a good idea to monitor Customer Support issues and social media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccfd8a9",
   "metadata": {},
   "source": [
    "## How to deal with an impact from Peeking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60422689",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Peeking is an issue of multiple test result calculations, the more you check the significance of your success metric\n",
    "## the more likely you are to see a false positive \n",
    "# can happen if you check the result too early \n",
    "## when we monitor the test, we only look and make decisions based on the critical business metrics (number of errors, crashes),\n",
    "## not the success metric\n",
    "# p-hacking is the practice of calculating the p-value up to the point where it reaches statistical significance\n",
    "## and basing the success of the test on that observation- not a good practice from an ethical standpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8c6ac",
   "metadata": {},
   "source": [
    "## Analysis of test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming we didn't have any major hiccups and we can analyse the data at the end of the test. These are a few steps:\n",
    "# 1. Look at pretest bias for the groups, look at overall metrics over time\n",
    "# 2. Observe whethere there is novelty effect and consider excluding the results from the test \n",
    "# 3. Calculate the relative and absolute difference \n",
    "# 4. Use our alpha (stastical significance level) and beta (test power) to determine whether test results are signficant or not \n",
    "# 5. Break down success metrics on their components- why sucess metrics met \n",
    "# 6. Share results with PMs, engineers, URs, and all other relevant stakeholders "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5b844",
   "metadata": {},
   "source": [
    "## Presenting your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice your presentation:\n",
    "# set the scene: feature info, test hypothesis, success metric, duration, group size, any assumptions made\n",
    "# mention how you chose and calculated the analysis metrics- from work or from take home assignment \n",
    "# state the test results and their reliability for each of the main metrics, statistical significance for test cases \n",
    "# talk about other interesting finding in the test e.g. data abnormal etc \n",
    "# talk about potential future analysis and tests- or improvements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how would you improve the results? \n",
    "# based on analysis of A/B test performance and the new feature "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb5444",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f13298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What could you improve in the test set up? more users assigned maybe\n",
    "# 2. Improve or change test monitoring process- theoretical as test done\n",
    "## 3. Improve the analysis part. Is there any more data that would be very valuable to look at?\n",
    "## in our case it could be link clicks, how much less over time do people click the link, how often do those links appear etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbde25",
   "metadata": {},
   "source": [
    "## What happens with scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more users brings almost instant statistical signficance as long as not time sensitive, but also bring new challenges\n",
    "# may also have to wait if 7 or 21 day retention \n",
    " # challenges include tests having more than one test case, multiple tests running at the same time \n",
    "    # different uses may have different experiences, so hard to differentiate between them\n",
    "    # determine test feature exposure - did users find feature easily? Look at users who went into settings menu\n",
    "    # might want to test interconnected features\n",
    "    ## if testing in different markets(+ marketing limitations)- may only be able to have a small control group or not \n",
    "    ## since marketing campaign may want a large number of userbase testing the feature  \n",
    "    # important to consider ethics and user discrmininations \n",
    "## you will asked to pull some data to satisy the curiosity of the people in your company and will need to be abe to tell them\n",
    "## what value of that question will be- could help advise them of better metric to use etc, \n",
    "## and if users are seeing things too often etc\n",
    "# to estimate the effect of that we also need to know our estimated revenue. Whether we will take a big hit or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a03170",
   "metadata": {},
   "source": [
    "## Analysis steps- how does an average A/B test analysis process look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. look at the assignments. Does the number of users represent the correct percent of the assignment split?\n",
    "## Are the number of daily/weekly etc assignments expected?\n",
    "# 2. Look at the pretest period or sucess metrics qnd non-inferiority metrics,\n",
    "## to make sure groups evenly distributed across different metrics. Is there any pretest bias or general bias?\n",
    "## 3. Look at the critical performance metrics. UX critical (crashes, availiability, etc)\n",
    "## and business critical(revenue, engagement)\n",
    "# 4. calculate the significance of the results on the Success Metric and Non-Inferiority Margin  \n",
    "# 5. Observe any temporal effects on the metrics. Is the impact stable or is there a novelty effect?\n",
    "## 6. Summaarise the results in an understandable, non-technical way, with the possibility to look at the assumptions made \n",
    "## and methods used for the analysis \n",
    "# 7. Summarise the suggestions for the next steps and potential additional research \n",
    "# 8. Share the results with stakeholders and data insights community, if there is one \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
