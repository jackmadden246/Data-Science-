{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feae8bb3",
   "metadata": {},
   "source": [
    "# Practical example. Audiobooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf8c64",
   "metadata": {},
   "source": [
    "## Preprocess the data. Balance the dataset. Create 3 datasets: training, validation and test. Save the newly created sets in a tensor friendly format (e.g. *.npz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949d2dd",
   "metadata": {},
   "source": [
    "Since we are dealing with real life data, we will need to process it a bit. This is the relevant code, which is not that hard, but refers to data engineering more than machine learning.\n",
    "\n",
    "If you want to know how to do that, go through the code and the commments. In any case, this should do the trick for all datasets organized in the way: many inputs, and then 1 cell containing the targets (all supervised learning datasets(.\n",
    "\n",
    "Note that we have removed the header row, which contains the names of the categories. We simply want the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100116b",
   "metadata": {},
   "source": [
    "## Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c631332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing ## should standardize inputs using sklearn, accuracy decreases by 10% otherwise \n",
    "raw_csv_data = np.loadtxt(\"Audiobooks_data.csv\", delimiter = \",\")\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1] ## takes all data except first and last columns (Id and target columns)\n",
    "targets_all = raw_csv_data[:,-1] ## creates variable for just targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8eddc5",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae2fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffled_indices = np.arange(unscaled_inputs_all.shape[0])\n",
    "#np.random.shuffle(shuffled_indices) ## put indices in variable and shuffle them \n",
    "\n",
    "#shuffled_inputs = unscaled_inputs_all[shuffled_indices]\n",
    "#shuffled_targets = targets_all[shuffled_indices] ## indices are shuffled indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f909e8",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30617d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = [] ## indices to be removed, want to be tuple or list, so leave empty for now with square brackets\n",
    "\n",
    "for i in range(targets_all.shape[0]): ## shape on 0 axis is basically length of vector so will show sum of targets \n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)            ## once number of zeros matches no of 1's, can note indices to be removed \n",
    "            \n",
    "            \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all,indices_to_remove,axis=0)\n",
    "# will delete content of indices to remove from inputs\n",
    "\n",
    "targets_equal_priors = np.delete(targets_all,indices_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c64987",
   "metadata": {},
   "outputs": [],
   "source": [
    "## important as could be more cats than dogs (model would think everything should be cats), or more 0's than 1's etc\n",
    "## we will count the number of targets that are 1's, and keep as many 0's as 1's (and delete the others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc602e",
   "metadata": {},
   "source": [
    "## Standardize the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2e03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors) # method standardizes array across each element, \n",
    "# imported from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d363d",
   "metadata": {},
   "source": [
    "## Shuffle the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c42e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices) ## put indices in variable and shuffle them \n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices] \n",
    "## indices are shuffled indices - oringinally here, now shuffling before balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13f0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now shuffle inputs and targets, same information but random order\n",
    "## need to shuffle sufficiently enough for batching to be used, e.g. in data order originally \n",
    "## like if a batch was homogenous, but between batches hetrogenous, \n",
    "## would confuse stochastic gradient descent when updating weights/loss after each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150ce36",
   "metadata": {},
   "source": [
    "## Split the dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a715bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1781.0 3579 0.49762503492595694\n",
      "231.0 447 0.5167785234899329\n",
      "225.0 448 0.5022321428571429\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0] ## going to split dataset into 3 datasets. 80,10,10 split\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count] # first train samples count of inputs\n",
    "train_targets = shuffled_targets[:train_samples_count] # first train samples count of targets\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count: train_samples_count + validation_samples_count] \n",
    "# between train and train samples, plus validation samples count\n",
    "validation_targets = shuffled_targets[train_samples_count: train_samples_count + validation_samples_count]\n",
    "# same place but for targets\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count + validation_samples_count:] \n",
    "# everything left after validation subset\n",
    "test_targets = shuffled_targets[train_samples_count + validation_samples_count:]\n",
    "\n",
    "print(np.sum(train_targets),train_samples_count,np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets),validation_samples_count,np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets),test_samples_count,np.sum(test_targets) / test_samples_count)\n",
    "## used to check if we have balanced training, test and validation datasets, not just whole dataset\n",
    "## prints number of 1's, number of samples, and proportion of 1's in respect of total, should all be around 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e01c35",
   "metadata": {},
   "source": [
    "## Save the 3 datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc676acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train',inputs = train_inputs, targets = train_targets)\n",
    "np.savez('Audiobooks_data_validation',inputs = validation_inputs,targets = validation_targets)\n",
    "np.savez('Audiobooks_data_test',inputs = test_inputs,targets = test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8c4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "## can use this for when having two classes, if more classes, need to balance datasheet for each class\n",
    "## proportions will change each time we run the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba1fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 50 is good rough amount for hidden layer size,\n",
    "## not too much which would slow learning, and see if we are actually learning anything\n",
    "## but not too few to add enough complexity \n",
    "## inputs is 10 since removing first column, and targets column, gives 10 inputs\n",
    "## 2 outputs due to 0 or 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
