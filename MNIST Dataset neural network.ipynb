{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfc01ca",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a653a3",
   "metadata": {},
   "source": [
    "We'll apply the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see. \n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digital recognition. You can find more about it on Yann LeCunn's website (Director of AI Research at Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as convolutiional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image)\n",
    "\n",
    "The goal is to writen an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "\n",
    "Our goal is to build a neural network with 2 hidden layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba6931",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91e0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394753ef",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9785e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised= True)\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "## mnist has no validation dataset, need to do that manually, set 10% of training dataset as validation dataset\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "## can count train samples or use mnist info- latter is readily availiable\n",
    "## divides samples by 10\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "## tf.cast converts samples to int to prefer float issues \n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples,tf.int64)\n",
    "## ideally like to scale data to have results be more numerically stable (e.g. inputs between 0 and 1)\n",
    "\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image /=255. ## . signifies we want output to be float\n",
    "    return image, label\n",
    "\n",
    "## function created to use on dataset.map, which uses function as input to a given dataset, \n",
    "## uses input as function to determine transformation\n",
    "## can use other methods instead as long as function uses image and label inputs and returns them\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "## need to shuffle data when preprocessing, same info but different order. Avoids 0's in one dataset, 1's in another etc\n",
    "## or higher prices in one, then low in another\n",
    "## helps with batching, to work as intended \n",
    "## one batch could have 0's, another 1's, causing loss to differ greatly\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "## take is used to extract sample size based on parameter, aka 10,000 for us \n",
    "## takes 10,000 observations, can't be too big because computer memory won't handle it \n",
    "## if buffer size set to 1, no shuffling happens \n",
    "## if buffer size >= num_samples, shuffling will happen at once (uniformly\n",
    "## if buffer size is between 1 and total sample size, we will be optimizing the computational power of computer \n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "## takes all samples but first 10% of samples, based on parameter \n",
    "\n",
    "## need to set batch size for batching\n",
    "## if batch size set to 1 = stoachastic gradient descent\n",
    "## if batch size # of samples, (single batch)GD as seen previously \n",
    "## need number reasonably small but large enough to preserve underlying dependencies = mini batch GD \n",
    "BATCH_SIZE = 100\n",
    "\n",
    "## batch method combines consecutive elements of dataset into batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "## indicates to model how many samples it should take in each batch \n",
    "## will override variable as we want a variable which is batched \n",
    "## don't need to create batching for validation dataset, \n",
    "## since batching reduces noise (meaningless or irrelevant data in real world data)\n",
    "## and validation dataset is forward propogated and only has 100 examples, compared to other datasets \n",
    "## batching is useful for updating weights once per batch, which is 100 samples, so reduce noise for training updates \n",
    "## we forward propogate once when validating\n",
    "## batching gives average loss, but we want actual loss when testing or validating so take all data at once.  \n",
    "## not expensive to calculate exact values, due to low computational power\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "## one batch \n",
    "test_data = test_data.batch(num_test_samples)\n",
    "## model expects validation dataset to be batched, so need to be overriden, same for test data \n",
    "## validation data must have same shape and object properties as train and test data- mnist data is iterable and 2-tuple format\n",
    "## as supervised set to true\n",
    "\n",
    "validation_inputs,validation_targets = next(iter(validation_data))\n",
    "## makes object iterable like a list, one element at a time, next loads next batch, \n",
    "## since only one batch, loads inputs and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a1ff9",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f642e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "## outline the model now data is loaded and preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf15286",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189f5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "## width and depth of model are hyperparemeters, can fiddle with them for improved result, \n",
    "## as 50 nodes for each hidden layer is suboptimal \n",
    "## 784 inputs for input layer, and 10 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b89f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 600\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "## assumption is all hidden layers are same size, but customised layers could be made to suit problem better\n",
    "model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation = 'softmax') ## when creating a classifier, \n",
    "                            ## activation function must change outputs into probabilities \n",
    "                            ## activation function is relu as specfcic to problem,\n",
    "                            ## each neural network irl has different optimal combination of activation functions \n",
    "                            ## builds layers of model, takes output size of next layer as argument \n",
    "                            ## takes inputs and calculates dot product, and calculates inputs and weights, \n",
    "                            ## can apply activation function too\n",
    "                             \n",
    "])  \n",
    "## our data from tfds is such that each input is 28 by 28 x 1 or tensor of rank 3\n",
    "## can use flatten object to turn tensor (images) into vector, takes argument shape of argument we want to flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e4ee4",
   "metadata": {},
   "source": [
    "## Choose optimizer and loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa97408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy',metrics = ['accuracy']) \n",
    "## adam optimizer and need loss function relevant to classifiers \n",
    "## there are 3 categorical cross entropy (best for categorical) loss functions for tensor flow. \n",
    "## Binary is for binary data, categorical presumes you've one hot encoded data, \n",
    "## while sparse categorical applies one-hot encoding\n",
    "## output shape need to match target shape, so need one-hot encoding  \n",
    "# can add metrics you want to calculate, so metrics for us is accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac443258",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b5f8778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.2094 - accuracy: 0.9364 - val_loss: 0.1040 - val_accuracy: 0.9682 - 5s/epoch - 10ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 4s - loss: 0.0798 - accuracy: 0.9750 - val_loss: 0.0755 - val_accuracy: 0.9760 - 4s/epoch - 8ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 4s - loss: 0.0545 - accuracy: 0.9824 - val_loss: 0.0500 - val_accuracy: 0.9837 - 4s/epoch - 8ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 4s - loss: 0.0376 - accuracy: 0.9879 - val_loss: 0.0390 - val_accuracy: 0.9882 - 4s/epoch - 7ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 4s - loss: 0.0331 - accuracy: 0.9894 - val_loss: 0.0325 - val_accuracy: 0.9895 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ed09574e20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "model.fit(train_data,epochs = NUM_EPOCHS, validation_data = (validation_inputs,validation_targets),callbacks = [early_stopping],\n",
    "          verbose = 2)\n",
    "          ## parametised it so can ammend the value if needed \n",
    "          ## like to create variables for specific things like epochs and output size, since we have hyperparameters, \n",
    "          ## and can spot it when debugging etc\n",
    "          ## NUM_EPOCHS = 5 , model.fit(train_data,epochs = NUM_EPOCHS. Enough to train model, but want to validate too\n",
    "          # only most important information obtained since verbose = 2 (determines lines of output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45aa0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. at beginning of each epoch training loss will be set to 0\n",
    "## 2. the alrogithm will iterate over a preset number of batches, all from train_data\n",
    "## 3. the weights and biases will be updated as many times as there are batches\n",
    "## 4. we will get a value for the loss function, indicating how training is going\n",
    "## 5. we will see the training accuracy \n",
    "## 6. At the end of the epoch, the algorithm will forward propogate through the validation set in single batch, \n",
    "## through optimized model, and calculate validation accuracy \n",
    "## when max epochs reached, training will be over \n",
    "## stochastic gradient descent very slow- my observations- less accurate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3419785",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss for this case is training loss, loss didn't change much \n",
    "## due to 540 weights and biases needed to be updated after first epoch\n",
    "## accuracy shows what percent of ouputs equal to targets \n",
    "## validation loss and accuracy are checks, keep an eye on validation loss to check if overfitting \n",
    "## validation accuracy is true accuracy of the model, since training accuracy is avwrage loss across batches, \n",
    "## while validation accuracy is for whole validation dataset\n",
    "## 97% for us first time \n",
    "##  can change hyperparemeters like number of hidden layer nodes (size)\n",
    "## hidden node size increased accuracy- 98.35 now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e7d31",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c8b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "## final accuracy is from forward propogating the test data, reason is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d2092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fine tuning hyperparemters can cause overfitting over validation training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6b45bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0671 - accuracy: 0.9797\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "## forward propogating the test data, returns loss and metrics value (accuracy for us) for the model in test mode\n",
    "## seperated them into two variables to be clear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98570592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.067. Test accuracy: 97.97%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2}. Test accuracy: {1:.2f}%'.format(test_loss,test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63f5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "## can't test it more than once, as first time model has not seen data, but other times it will have and could overfit\n",
    "## main point of test dataset is to simulate model deployment \n",
    "## low accuracy means model has overfit, but accuracy close to validation accuracy shows we have not overfit \n",
    "## based on what we would expect if put it in real life "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
